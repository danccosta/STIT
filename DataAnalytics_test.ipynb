{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eec3119",
   "metadata": {},
   "source": [
    "# ST IT Cloud - Data and Analytics Test LV.4\n",
    "\n",
    "Esse teste deve avaliar alguns conceitos de big data e a qualidade técnica na manipulacão de dados, otimização de performance, trabalho com arquivos grandes e tratamento de qualidade.\n",
    "\n",
    "## Passo a passo\n",
    "\n",
    "- *Parte teórica:* responda as questões abaixo preenchendo as células em branco.\n",
    "- *Parte prática:* disponibilizamos aqui 2 cases para, leia os enunciados dos problemas, desenvolver os programas, utilizando a **stack definida durante o processo seletivo**, para entregar os dados de acordo com os requisitos descritos abaixo.\n",
    "\n",
    "\n",
    "\n",
    "**Faz parte dos critérios de avaliacão a pontualidade da entrega. Implemente até onde for possível dentro do prazo acordado.**\n",
    "\n",
    "**Os dados de pessoas foram gerados de forma aleatória, utilizando a biblioteca FakerJS, FakerJS-BR e Faker**\n",
    "\n",
    "LEMBRE-SE: A entrega deve conter TODOS os passos para o avaliador executar o programa (keep it simple).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447dec4",
   "metadata": {},
   "source": [
    "**Questão 1** - Descreva de forma detalhada quais são as etapas na construção de um pipeline de dados, sem considerar ferramentas específicas, imagine que é seu primeiro contato com o cliente e você precisa entender a demanda dele e explicar quais são os passos que você terá que implementar para entregar a demanda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d5b727",
   "metadata": {},
   "source": [
    "Um framework começa pela extração dos dados na origem, logo é necessário definir quais tabelas serão utilizadas, qual será o intervalo de extração, se a carga será completa ou incremental e providenciar a conexão dos bancos de dados com os componentes do framework. Em seguida, os dados devem ser organizados no Data Lake de forma a otimizar a leitura e separados em camadas. Os dados na primeira camada podem necessitar de um pré processamento para remoção de valores vazios, duplicados, filtrar por data ou até alterar o schema da tabela. Com os dados limpos, o time de analistas de negócio em conjunto com o de modelagem de dados devem definir quais regras de negócio devem ser aplicadas para a formação de novos conjuntos de dados, cabendo ao time de engenharia de dados implementar tais regras. O próximo passo é carregar os dados no Data Warehouse, podendo ser segregados em Data Marts a depender da necessidade do cliente para facilitar o controle de acesso aos mesmos. Com a conclusão do carregamento no DW, os dados estarão disponíveis para a confecção de relatórios e dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc703af3",
   "metadata": {},
   "source": [
    "**Questão 2** - Defina com suas palavras um processamento em streaming e processamento em batch. Qual sua experiência com cada uma delas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd4322",
   "metadata": {},
   "source": [
    " - Streaming - Processamento de dados em tempo real, ideal para coleta de dados de sensores e IoT\n",
    " - Batch - Processamento de dados sem necessidade de tempo real, onde uma grande carga é realizada em intervalos pré definidos de acordo com a necessidade do cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c3f10",
   "metadata": {},
   "source": [
    "**Questão 3** - Quais são as camadas de um Data Lake?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6cb7b4",
   "metadata": {},
   "source": [
    "Um Data Lake pode ter diversas camadas, de acordo com a necessidade do cliente. Habitualmente há pelo menos uma camada de dados \"Raw\" onde os dados brutos são armazenados, uma camada intermediária \"Refinement\" para armazenamento dos dados pré processados e uma camada final \"Publish\" para os dados prontos para o carregamento no DW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62477089",
   "metadata": {},
   "source": [
    "**Questão 4** - Quais as diferenças de um Data Lake e um DW?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9a685",
   "metadata": {},
   "source": [
    " - Data Lake - Armazenamento de dados em formato de arquivo para fins de tratamento e arquivamento\n",
    " - Data Warehouse - Armazenamento de dados em formato tabular para fins de geração de relatórios e dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099a05e",
   "metadata": {},
   "source": [
    "**Questão 5** - O que é arquitetura Lambda e Kappa? Descreva com suas palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d747fd",
   "metadata": {},
   "source": [
    " - Lambda - Arquitetura de framework onde há dois \"caminhos\" para os dados, um caminho para dados em tempo real e outro para dados batch\n",
    " - Kappa - Arquitetura de framework onde há apenas um caminho rápido tanto para dados de tempo real quanto em lotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d483e",
   "metadata": {},
   "source": [
    "**Questão 6** - O que é Data Quality para você e como você implementa isso nos seus processos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0cd15",
   "metadata": {},
   "source": [
    "Data Quality é assegurar a qualidade dos dados e que o resultado seja aquilo que se espera. Em meu projeto atual, criamos um notebook que executa um row count das tabelas entre as camadas do Data Lake, procura por valores duplicados e verifica se o schema das tabelas não é modificado sem o nosso conhecimento. Após um tempo trabalhando dessa forma, um time de Testers assumiu essa tarefa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4834d6",
   "metadata": {},
   "source": [
    "**Questão 7** - Em uma escala de 0 a 10, qual seria seu nível de experiência com PySpark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1246d2b4",
   "metadata": {},
   "source": [
    "8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef78ba",
   "metadata": {},
   "source": [
    "**Questão 8** - Em uma escala de 0 a 10, qual seria seu nível de experiência com SQL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe47658",
   "metadata": {},
   "source": [
    "8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f7ee6",
   "metadata": {},
   "source": [
    "**Questão 9** - Descreva suas expeciências com banco de dados SQL e NoSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebbaccc",
   "metadata": {},
   "source": [
    " - Dados SQL - São utilizados em bancos de dados relacionais, dados estruturados e armazenados em formato tabular com estrutura pré definida. Aceita apenas dados homogêneos, tende a possuir lógica mais simples, porém apresenta maior complexidade em escalabilidade.\n",
    " - Dados NoSQL (Not only SQL) - São utilizados em bancos de dados não relacionais, com dados de diferentes formatos e estrutura heterogênea (imagens, textos, etc). Apresentam menor complexidade para escalabilidade, são úteis para algoritmos de Machine Learning, entretanto acabam exigindo uma lógica mais complexa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fa6e5",
   "metadata": {},
   "source": [
    "**Questão 10** - Tem experiência com versionamento de código? Com quais ferramentas já trabalhou? Descreva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb9a2d",
   "metadata": {},
   "source": [
    "Sim, trabalhei com GitLab e atualmente com GitBucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2714f",
   "metadata": {},
   "source": [
    "**Questão 11** - Tem experiência em desenvolvimento em cloud? Se sim, especifique a(s) plataforma(s) que já trabalhou e suas principais implementações e conhecimentos em cada serviço."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318edd6",
   "metadata": {},
   "source": [
    "Sim, tenho certificações Azure (Fundamentals) e AWS (Practitioner, SysOps e Architect Associate), sendo minha experiência mais sólida em AWS. Meu primeiro projeto em Big Data foi na implementação de um framework composto apenas por serviços AWS. A extração dos dados era executada por instancias EC2 com NiFi, S3 como Data Lake, Redshift como DW, transformações com Glue, Funções Lambda e EMR e Step Function como orquestrador. Atualmente implemento um framework na AWS cujo Data Lake também é S3, porém o DW é Snowflake, transformações executadas por cluster Databricks, extração através do StreamSets e Airflow com orquestrador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bc70d",
   "metadata": {},
   "source": [
    "**Questão 12** - Tem experiência com metodologia ágil? Qual?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e09ec3",
   "metadata": {},
   "source": [
    "Sim, trabalho em um squad com mais 3 desenvolvedores, usamos o Kanban do Jira para distribuir as atividades entre a equipe e realizamos reuniões diárias no início do dia (15 min) para atualizar o andamento das nossas tarefas e se temos algum impedimento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb61f06",
   "metadata": {},
   "source": [
    "# TESTE PRÁTICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8c6a5",
   "metadata": {},
   "source": [
    "**Problema 1**: Você está recebendo o arquivo 'dados_cadastrais_fake.csv' que contem dados cadastrais de clientes, mas para que análises ou relatórios sejam feitos é necessário limpar e normalizar os dados. Além disso, existe uma coluna com o número de cpf e outra com cnpj, você precisará padronizar deixando apenas dígitos em formato string (sem caracteres especiais), implementar uma forma de verificar se tais documentos são válidos sendo que a informação deve se adicionada ao dataframe em outras duas novas colunas.\n",
    "\n",
    "Após a normalização, gere reports que respondam as seguintes perguntas:\n",
    "- Quantos clientes temos nessa base?\n",
    "- Qual a média de idade dos clientes?\n",
    "- Quantos clientes nessa base pertencem a cada estado?\n",
    "- Quantos CPFs válidos e inválidos foram encontrados?\n",
    "- Quantos CNPJs válidos e inválidos foram encontrados?\n",
    "\n",
    "Ao final gere um arquivo no formato csv e um outro arquivo no formato parquet chamado (problema1_normalizado), eles serão destinados para pessoas distintas.\n",
    "\n",
    "*EXTRA:* executar as mesmas validações no *1E8.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291cd9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff9c0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a75b0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(header=True,sep=\";\").csv(\"dados_cadastrais_fake.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1386adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def validate_code(string: str):\n",
    "    numeric = \"\"\n",
    "    for digit in string:\n",
    "        if digit.isdigit():\n",
    "            numeric += digit\n",
    "        else: continue\n",
    "    return numeric\n",
    "\n",
    "def is_cpf_valid(string: str):\n",
    "    if len(string) == 11:\n",
    "        return True\n",
    "    else: return False\n",
    "\n",
    "def is_cnpj_valid(string: str):\n",
    "    if len(string) == 14:\n",
    "        return True\n",
    "    else: return False\n",
    "\n",
    "validate_code_udf = udf(lambda z: validate_code(z),StringType())\n",
    "is_cpf_valid_udf = udf(lambda z: is_cpf_valid(z),BooleanType())\n",
    "is_cnpj_valid_udf = udf(lambda z: is_cnpj_valid(z),BooleanType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c20ec7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-------------------+----------------+-----------+--------------+---------+----------+\n",
      "|             nomes|idade|             cidade|          estado|        cpf|          cnpj|valid_cpf|valid_cnpj|\n",
      "+------------------+-----+-------------------+----------------+-----------+--------------+---------+----------+\n",
      "|    Dennis Daniels|   31|         ACRELÂNDIA|              AC|97566536800|06589184909526|     true|      true|\n",
      "|       Leah Becker|   42|        ÁGUA BRANCA|              AL|42526380707|25673336235020|     true|      true|\n",
      "|        Sally Ford|   18|           ALVARÃES|              AM|34647754103|26543101702989|     true|      true|\n",
      "|    Colleen Duncan|   21|     SERRA DO NAVIO|              AP|25253156003|19062080510098|     true|      true|\n",
      "|   Jeff Stephenson|   73|             ABAÍRA|              BA|49668886542|97794530015384|     true|      true|\n",
      "|     Sydney Curtis|   85|            ABAIARA|              CE|50620290749|29476298085678|     true|      true|\n",
      "|    Kelly Matthews|   44|           Brasília|distrito federal|39154836808|24709301957761|     true|      true|\n",
      "|         Juan Ruiz|   39|     AFONSO CLÁUDIO|              ES|22688119648|02420338147900|     true|      true|\n",
      "|      Brian Thomas|   26|    ABADIA DE GOIÁS|              GO|47475484084|70723419110335|     true|      true|\n",
      "|        Sara Ayers|   62|         AÇAILÂNDIA|              MA|94830978864|88253689548382|     true|      true|\n",
      "|        Brady Cruz|   83|ABADIA DOS DOURADOS|              MG|03438452456|13118105069639|     true|      true|\n",
      "|   Samantha Wright|   32|         ÁGUA CLARA|              MS|22203654090|04310317320252|     true|      true|\n",
      "|    Richard Turner|   85|           ACORIZAL|              MT|79059712285|75595541223248|     true|      true|\n",
      "|    Ashley Sanders|   73|         ABAETETUBA|              PA|75206513989|06923815165128|     true|      true|\n",
      "|      Thomas Jones|   45|        ÁGUA BRANCA|              PB|34565179806|26574533273304|     true|      true|\n",
      "|        Ian Murray|   68|       ABREU E LIMA|              PE|15277717099|85202887368851|     true|      true|\n",
      "|     Janice Jensen|   79|              ACAUÃ|              PI|45081993646|89596067898809|     true|      true|\n",
      "|Jeffrey Cunningham|   48|             ABATIÁ|              PR|66549028825|18866688344242|     true|      true|\n",
      "|      Amanda Glass|   49|     ANGRA DOS REIS|              RJ|86173803577|87380936406714|     true|      true|\n",
      "|   Kaitlyn Johnson|   65|              ACARI|              RN|89737004213|40893546741494|     true|      true|\n",
      "+------------------+-----+-------------------+----------------+-----------+--------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_val = df.withColumn(\"cpf\", validate_code_udf(col(\"cpf\"))) \\\n",
    "           .withColumn(\"cnpj\", validate_code_udf(col(\"cnpj\"))) \\\n",
    "           .withColumn(\"valid_cpf\", is_cpf_valid_udf(col(\"cpf\"))) \\\n",
    "           .withColumn(\"valid_cnpj\", is_cnpj_valid_udf(col(\"cnpj\")))\n",
    "df_val.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5cc9e55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|nomes|\n",
      "+-----+\n",
      "|10000|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quantos clientes temos nessa base?\n",
    "df_val.dropDuplicates().summary(\"count\").select(\"nomes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9ed6f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  idade|\n",
      "+-------+\n",
      "|53.7831|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Qual a média de idade dos clientes?\n",
    "df_val.dropDuplicates().summary(\"mean\").select(\"idade\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1146b291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|estado|count(nomes)|\n",
      "+------+------------+\n",
      "|    SC|         370|\n",
      "|    RO|         370|\n",
      "|    PI|         370|\n",
      "|    AM|         371|\n",
      "|    RR|         370|\n",
      "|    GO|         371|\n",
      "|    TO|         370|\n",
      "|    MT|         370|\n",
      "|    SP|         370|\n",
      "|    PB|         370|\n",
      "|    ES|         371|\n",
      "|    RS|         370|\n",
      "|    MS|         370|\n",
      "|    AL|         371|\n",
      "|    MG|         370|\n",
      "|    PA|         370|\n",
      "|    BA|         371|\n",
      "|    SE|         370|\n",
      "|    PE|         370|\n",
      "|    CE|         371|\n",
      "+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quantos clientes nessa base pertencem a cada estado?\n",
    "df_res = df_val.dropDuplicates() \\\n",
    "               .withColumn(\"estado\", regexp_replace(trim(col(\"estado\")), \"são  paulo\", \"SP\")) \\\n",
    "               .withColumn(\"estado\", regexp_replace(trim(col(\"estado\")), \"sao  paulo\", \"SP\")) \\\n",
    "               .withColumn(\"estado\", regexp_replace(trim(col(\"estado\")), \"MINAS GERAIs\", \"MG\")) \\\n",
    "               .withColumn(\"estado\", regexp_replace(trim(col(\"estado\")), \"MINAS GERAI\", \"MG\")) \\\n",
    "               .withColumn(\"estado\", regexp_replace(trim(col(\"estado\")), \"rio de  janeiro\", \"RJ\")) \\\n",
    "               .withColumn(\"estado\", regexp_replace(trim(col(\"estado\")), \"distrito federal\", \"DF\"))\n",
    "\n",
    "df_res.groupBy(\"estado\").agg(count(\"nomes\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f54228c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|valid_cpf|count(cpf)|\n",
      "+---------+----------+\n",
      "|     true|     10000|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quantos CPFs válidos e inválidos foram encontrados?\n",
    "# Critério para cpf válido: 11 dígitos numéricos\n",
    "# Contagem realizada após tratamento da coluna cpf\n",
    "df_res.groupBy(\"valid_cpf\").agg(count(\"cpf\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3a9707df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|valid_cnpj|count(cnpj)|\n",
      "+----------+-----------+\n",
      "|      true|      10000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quantos CNPJs válidos e inválidos foram encontrados?\n",
    "# Critério para cnpj válido: 14 dígitos numéricos\n",
    "# Contagem realizada após tratamento da coluna cnpj\n",
    "df_res.groupBy(\"valid_cnpj\").agg(count(\"cnpj\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da9c40",
   "metadata": {},
   "source": [
    "**Problema 2**: Você deverá implementar um programa, para ler, tratar e particionar os dados.\n",
    "\n",
    "O arquivo fonte está disponível em `https://st-it-cloud-public.s3.amazonaws.com/people-v2_1E6.csv.gz`\n",
    "\n",
    "### Data Quality\n",
    "\n",
    "- Higienizar e homogenizar o formato da coluna `document`\n",
    "- Detectar através da coluna `document` se o registro é de uma Pessoa Física ou Pessoa Jurídica, adicionando uma coluna com essa informação\n",
    "- Higienizar e homogenizar o formato da coluna `birthDate`\n",
    "- Existem duas colunas nesse dataset que em alguns registros estão trocadas. Quais são essas colunas? \n",
    "- Corrigir os dados com as colunas trocadas\n",
    "- Além desses pontos, existem outras tratamentos para homogenizar esse dataset. Aplique todos que conseguir.\n",
    "\n",
    "### Agregação dos dados\n",
    "\n",
    "- Quais são as 5 PF que mais gastaram (`totalSpent`)? \n",
    "- Qual é o valor de gasto médio por estado (`state`)?\n",
    "- Qual é o valor de gasto médio por `jobArea`?\n",
    "- Qual é a PF que gastou menos (`totalSpent`)?\n",
    "- Quantos nomes e documentos repetidos existem nesse dataset?\n",
    "- Quantas linhas existem nesse dataset?\n",
    "\n",
    "### Particionamento de dados tratados com as regras descritas em `DATA QUALITY`\n",
    "\n",
    "- Particionar em arquivos PARQUET por estado (`state`)\n",
    "- Particionar em arquivos CSV por ano/mes/dia de nascimento (`birthDate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2277f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(header=True,sep=\";\").csv(\"people-v2_1E6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1931d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higienizar e homogenizar o formato da coluna document\n",
    "# Detectar através da coluna document se o registro é de uma Pessoa Física ou Pessoa Jurídica, adicionando uma coluna com essa informação\n",
    "\n",
    "df2 = df.withColumn(\"document\", validate_code_udf(trim(col(\"document\")))) \\\n",
    "  .withColumn(\"PF_PJ\", when(length(col(\"document\")) == 11, lit(\"PF\")) \\\n",
    "                      .when(length(col(\"document\")) == 14, lit(\"PJ\"))\n",
    "                      .otherwise(\"not_valid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b09a0122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+-------------------+------------+------------------+----------+--------------------+------------------+----------+-----+\n",
      "|      document|                name|                 job|            jobArea|     jobType|       phoneNumber| birthDate|                city|             state|totalSpent|PF_PJ|\n",
      "+--------------+--------------------+--------------------+-------------------+------------+------------------+----------+--------------------+------------------+----------+-----+\n",
      "|   76684148787|     Charlleny Braga|Oficial Criativo ...|       Configuração|Estrategista|    (62) 4216-9799|1972-05-20|   Município de Iara|             Goiás|    913.80|   PF|\n",
      "|   85704855733|      Newton Saraiva|Administrador Com...|Prestação de contas| Facilitador|        Aplicações|1982-06-10|Município de Neid...|                RR|     57.26|   PF|\n",
      "|15664328373377|Dr. Sr. Solange M...|Designer Identida...|           Métricas|        null|+55 (95) 7143-3307|1968-05-16|  Município de Santo|                RO|    660.71|   PJ|\n",
      "|02328238087786| Celina Carvalho Jr.|                null|          Qualidade|     Ligação|+55 (58) 4136-5577|      null|       Reis do Norte|                RO|    542.51|   PJ|\n",
      "|30073687408740|      Aurilo Martins|Especialista Para...|           Programa|   Executivo|   (96) 47498-7325|1980-04-07|Município de Amin...|                RO|    104.33|   PJ|\n",
      "|   77145788233|Sr. Sávio Albuque...|Estrategista Bran...|           Resposta|  Planejador|+55 (06) 0512-5291|      null| Município de Balmes|           Sergipe|    769.26|   PF|\n",
      "|   75752983045|Sr. Everaldo Sara...|Técnico Táticas P...|          Marketing|     Diretor|   (33) 78784-4156|1980-10-27|    Nova Olga do Sul|      Minas Gerais|     575.1|   PF|\n",
      "|   88363035521|     Martina Moreira|Associado Contas ...|          Marketing|    Designer|+55 (30) 1195-8461|1978-09-08|Reis de Nossa Sen...|                BA|    381.15|   PF|\n",
      "|   46182752785|Benedikt-Jan Pereira|Associado Contas ...|             Contas|     Gerente|+55 (36) 8023-5165|      null|         Melo do Sul|        Pernambuco|    496.97|   PF|\n",
      "|   15111381490|   Izamar Melo Filho|Designer Configur...|           Mercados|Especialista|   (37) 02302-7101|1987-01-19|                null|Mato Grosso do Sul|    400.18|   PF|\n",
      "|   96857779341|     Débora Oliveira|Associado Seguran...|      Implementação|     Oficial|   (02) 63755-0226|      null|       Balmes do Sul|                SP|    473.01|   PF|\n",
      "|   58412777743|Sra. Lhirton Bati...|Produtor Intranet...|       Configuração|   Arquiteto|   (33) 70042-1213|1985-05-12|      Batista do Sul|                PA|    300.22|   PF|\n",
      "|43256426510740| Sra. Rosildo Santos|Desenvolvedor Fun...|         Integração|  Engenheiro|    (12) 3014-4120|      null|                null|              Pará|    658.82|   PJ|\n",
      "|   23285262680|    Dov Carvalho Jr.|Administrador Pes...|           Resposta|    Analista|   (65) 37328-4182|1989-08-02|      Nova Elisabete|         Tocantins|    795.89|   PF|\n",
      "|23487182365693|Edith Albuquerque...|Diretor Interaçõe...|           Garantia|Estrategista|+55 (67) 9793-9050|1986-12-02|                null|              Pará|    286.44|   PJ|\n",
      "|   15174710520|Deilson Santos Filho|Estrategista Qual...|           Garantia|    Produtor|    (70) 7104-0331|1963-08-18|       Nova Michelle|           Sergipe|    249.36|   PF|\n",
      "|   04556513537|       Viviana Braga|                null|          Diretivas|   Associado|   (10) 11373-1986|      null|                null|              Acre|    920.21|   PF|\n",
      "|   67445640053|      Dione Oliveira|Facilitador Soluç...|Prestação de contas|        null|   (67) 75494-6612|1987-04-19|      Pereira do Sul|           Sergipe|    768.23|   PF|\n",
      "|   74854451114|          Kauy Filho|Supervisor Qualid...|         Otimização|        null|    (07) 8716-5981|1978-12-07|Nova Evany de Nos...|                PE|    393.67|   PF|\n",
      "|13243665857133|  Dr. Valdete Franco|                null|         Integração|        null|+55 (21) 6949-7240|1972-03-21|Grande Jeferson d...|         Tocantins|    773.84|   PJ|\n",
      "+--------------+--------------------+--------------------+-------------------+------------+------------------+----------+--------------------+------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Higienizar e homogenizar o formato da coluna birthDate\n",
    "\n",
    "from pyspark.sql.functions import coalesce, to_date\n",
    "\n",
    "def to_date_(col, formats=(\"d-M-y\", \"M/d/y\", \"d-MMM-y\", \"yMd\", \"MMM/d/y\")):\n",
    "    return coalesce(*[to_date(col, f) for f in formats])\n",
    "\n",
    "df3 = df2.withColumn(\"birthDate\", to_date_(\"birthDate\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d90333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+-------------------+------------+------------------+----------------+--------------------+------------------+----------+-----+\n",
      "|      document|                name|                 job|            jobArea|     jobType|       phoneNumber|       birthDate|                city|             state|totalSpent|PF_PJ|\n",
      "+--------------+--------------------+--------------------+-------------------+------------+------------------+----------------+--------------------+------------------+----------+-----+\n",
      "|   76684148787|     Charlleny Braga|Oficial Criativo ...|       Configuração|Estrategista|    (62) 4216-9799|      20-05-1972|   Município de Iara|             Goiás|    913.80|   PF|\n",
      "|   85704855733|      Newton Saraiva|Administrador Com...|Prestação de contas| Facilitador|              null|     10-Jun-1982|Município de Neid...|                RR|     57.26|   PF|\n",
      "|15664328373377|Dr. Sr. Solange M...|Designer Identida...|           Métricas|        null|+55 (95) 7143-3307|      05/16/1968|  Município de Santo|                RO|    660.71|   PJ|\n",
      "|02328238087786| Celina Carvalho Jr.|                null|          Qualidade|     Ligação|+55 (58) 4136-5577|       19810417,|       Reis do Norte|                RO|    542.51|   PJ|\n",
      "|30073687408740|      Aurilo Martins|Especialista Para...|           Programa|   Executivo|   (96) 47498-7325|      04/07/1980|Município de Amin...|                RO|    104.33|   PJ|\n",
      "|   77145788233|Sr. Sávio Albuque...|Estrategista Bran...|           Resposta|  Planejador|+55 (06) 0512-5291|       19830618,| Município de Balmes|           Sergipe|    769.26|   PF|\n",
      "|   75752983045|Sr. Everaldo Sara...|Técnico Táticas P...|          Marketing|     Diretor|   (33) 78784-4156|     27-Oct-1980|    Nova Olga do Sul|      Minas Gerais|     575.1|   PF|\n",
      "|   88363035521|     Martina Moreira|Associado Contas ...|          Marketing|    Designer|+55 (30) 1195-8461|      09/08/1978|Reis de Nossa Sen...|                BA|    381.15|   PF|\n",
      "|   46182752785|Benedikt-Jan Pereira|Associado Contas ...|             Contas|     Gerente|+55 (36) 8023-5165|       19771120,|         Melo do Sul|        Pernambuco|    496.97|   PF|\n",
      "|   15111381490|   Izamar Melo Filho|Designer Configur...|           Mercados|Especialista|   (37) 02302-7101|      19-01-1987|                null|Mato Grosso do Sul|    400.18|   PF|\n",
      "|   96857779341|     Débora Oliveira|Associado Seguran...|      Implementação|     Oficial|   (02) 63755-0226|Thu, Jan.30.1975|       Balmes do Sul|                SP|    473.01|   PF|\n",
      "|   58412777743|Sra. Lhirton Bati...|Produtor Intranet...|       Configuração|   Arquiteto|   (33) 70042-1213|     May/12/1985|      Batista do Sul|                PA|    300.22|   PF|\n",
      "|43256426510740| Sra. Rosildo Santos|Desenvolvedor Fun...|         Integração|  Engenheiro|    (12) 3014-4120|       19801214,|                null|              Pará|    658.82|   PJ|\n",
      "|   23285262680|    Dov Carvalho Jr.|Administrador Pes...|           Resposta|    Analista|   (65) 37328-4182|     Aug/02/1989|      Nova Elisabete|         Tocantins|    795.89|   PF|\n",
      "|23487182365693|Edith Albuquerque...|Diretor Interaçõe...|           Garantia|Estrategista|+55 (67) 9793-9050|     Dec/02/1986|                null|              Pará|    286.44|   PJ|\n",
      "|   15174710520|Deilson Santos Filho|Estrategista Qual...|           Garantia|    Produtor|    (70) 7104-0331|      08/18/1963|       Nova Michelle|           Sergipe|    249.36|   PF|\n",
      "|   04556513537|       Viviana Braga|                null|          Diretivas|   Associado|   (10) 11373-1986|       19820810,|                null|              Acre|    920.21|   PF|\n",
      "|   67445640053|      Dione Oliveira|Facilitador Soluç...|Prestação de contas|        null|   (67) 75494-6612|     19-Apr-1987|      Pereira do Sul|           Sergipe|    768.23|   PF|\n",
      "|   74854451114|          Kauy Filho|Supervisor Qualid...|         Otimização|        null|    (07) 8716-5981|     Dec/07/1978|Nova Evany de Nos...|                PE|    393.67|   PF|\n",
      "|13243665857133|  Dr. Valdete Franco|                null|         Integração|        null|+55 (21) 6949-7240|      21-03-1972|Grande Jeferson d...|         Tocantins|    773.84|   PJ|\n",
      "+--------------+--------------------+--------------------+-------------------+------------+------------------+----------------+--------------------+------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Existem duas colunas nesse dataset que em alguns registros estão trocadas. Quais são essas colunas? R: jobArea e phoneNumber\n",
    "# Corrigir os dados com as colunas trocadas\n",
    "\n",
    "df4 = df2.withColumn(\"jobArea_tmp\", col(\"jobArea\")) \\\n",
    "         .withColumn(\"phoneNumber_tmp\", col(\"phoneNumber\")) \\\n",
    "         .withColumn(\"jobArea\", when(length(validate_code_udf(col(\"jobArea_tmp\"))) > 1, lit(None)) \\\n",
    "                     .otherwise(col(\"jobArea_tmp\"))) \\\n",
    "         .withColumn(\"phoneNumber\", when(length(validate_code_udf(col(\"phoneNumber_tmp\"))) == 0, lit(None)) \\\n",
    "                     .otherwise(col(\"phoneNumber_tmp\"))) \\\n",
    "         .drop(\"jobArea_tmp\", \"phoneNumber_tmp\")\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30b470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------+\n",
      "|              name|   document|totalSpent_sum|\n",
      "+------------------+-----------+--------------+\n",
      "| Euvanderson Costa|82190864755|        1000.0|\n",
      "|Sra. Rocio Martins|18741688813|        1000.0|\n",
      "|    Wandir Martins|61454551445|        999.99|\n",
      "|       Regina Melo|17195012700|        999.99|\n",
      "|     Valeria Souza|58953147670|        999.99|\n",
      "+------------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quais são as 5 PF que mais gastaram (totalSpent)?\n",
    "\n",
    "df4.filter(\"PF_PJ == 'PF'\") \\\n",
    "    .groupBy(\"name\",\"document\") \\\n",
    "    .agg(sum(\"totalSpent\").alias(\"totalSpent_sum\")) \\\n",
    "    .orderBy(col(\"totalSpent_sum\").desc()) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e000be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|state|        spent_mean|\n",
      "+-----+------------------+\n",
      "|   SC| 501.3456124521237|\n",
      "|   RO|  498.237317849353|\n",
      "|   PI| 499.0765211522301|\n",
      "|   AM|498.77066298058446|\n",
      "|   RR|500.95639783978476|\n",
      "|   GO| 501.2924926083047|\n",
      "|   TO| 501.9015633481716|\n",
      "|   MT| 500.2646100271407|\n",
      "|   SP| 498.6944797150817|\n",
      "|   PB|499.20039918878786|\n",
      "|   ES| 501.6317474781182|\n",
      "|   BH|497.75054104477545|\n",
      "|   RS|500.63098342994965|\n",
      "|   MS|499.50974141011864|\n",
      "|   AL|500.51551942023036|\n",
      "|   MG|499.53209412922274|\n",
      "|   PA|   499.23084708669|\n",
      "|   BA| 499.6714021164028|\n",
      "|   SE| 498.2400992515452|\n",
      "|   PE|502.40577268067716|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Qual é o valor de gasto médio por estado (state)?\n",
    "\n",
    "df5 = df4.dropDuplicates() \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Santa Catarina\", \"SC\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Mato Grosso do Sul\", \"MS\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Goiás\", \"GO\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Mato Grosso\", \"MT\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Ceará\", \"CE\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Espírito Santo\", \"ES\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Piauí\", \"PI\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Paraná\", \"PR\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Alagoas\", \"AL\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Bahia\", \"BH\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Roraima\", \"RR\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Distrito Federal\", \"DF\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Pernambuco\", \"PE\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Amazonas\", \"AM\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Rio Grande do Sul\", \"RS\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Acre\", \"AC\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Rio Grande do Norte\", \"RN\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Sergipe\", \"SE\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"São Paulo\", \"SP\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Rio de Janeiro\", \"RJ\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Minas Gerais\", \"MG\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Rondônia\", \"RO\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Maranhão\", \"MA\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Tocantins\", \"TO\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Paraíba\", \"PB\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Pará\", \"PA\")) \\\n",
    "               .withColumn(\"state\", regexp_replace(trim(col(\"state\")), \"Amapá\", \"AP\"))\n",
    "\n",
    "df5.groupBy(\"state\").agg(mean(\"totalSpent\").alias(\"spent_mean\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e17ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|            jobArea|        spent_mean|\n",
      "+-------------------+------------------+\n",
      "|          Paradigma| 502.9419944446624|\n",
      "|      Implementação| 501.6488623650825|\n",
      "|              Grupo| 500.6132407150521|\n",
      "|           Soluções| 501.4326185234694|\n",
      "|         Identidade|  498.906987072479|\n",
      "|           Branding|499.80194382679747|\n",
      "|            Táticas| 498.4137747645642|\n",
      "|       Configuração| 502.3573366375891|\n",
      "|Prestação de contas| 498.8216184834126|\n",
      "|               null| 500.2928956194941|\n",
      "|         Interações|498.79547197349274|\n",
      "|           Mercados|  495.435343256618|\n",
      "|           Pesquisa| 498.8122161229603|\n",
      "|           Garantia| 497.8645700630693|\n",
      "|               Rede| 500.9353493279145|\n",
      "|   A infraestrutura| 499.6950264508801|\n",
      "|           Métricas|499.13984875935444|\n",
      "|             Contas| 499.0756250979169|\n",
      "|           Intranet| 499.7327174212875|\n",
      "|              Dados| 500.8420806464449|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Qual é o valor de gasto médio por jobArea?\n",
    "\n",
    "df5.groupBy(\"jobArea\").agg(mean(\"totalSpent\").alias(\"spent_mean\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c6ed8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------------+\n",
      "|          name|   document|totalSpent_sum|\n",
      "+--------------+-----------+--------------+\n",
      "|Sr. Enio Souza|58868926210|           0.0|\n",
      "+--------------+-----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Qual é a PF que gastou menos (totalSpent)?\n",
    "# Não ficou claro se há algum critério de desempate\n",
    "\n",
    "df5.filter(\"PF_PJ == 'PF'\") \\\n",
    "    .filter(\"totalSpent is not Null\") \\\n",
    "    .groupBy(\"name\",\"document\") \\\n",
    "    .agg(sum(\"totalSpent\").alias(\"totalSpent_sum\")) \\\n",
    "    .orderBy(col(\"totalSpent_sum\")) \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d32c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454946"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantos nomes e documentos repetidos existem nesse dataset?\n",
    "# Aplicando a função em cadeia o resultado fica 1000000 - 454946 = 545054\n",
    "\n",
    "df.dropDuplicates([\"document\"]).dropDuplicates([\"name\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "583b59d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantas linhas existem nesse dataset?\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43213b90",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o277.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:240)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:240)\r\n\t... 42 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10004/1062013183.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Particionar em arquivos PARQUET por estado (state)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"state\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"overwrite\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/User01/ST-IT/teste_output_parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1248\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o277.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:240)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:240)\r\n\t... 42 more\r\n"
     ]
    }
   ],
   "source": [
    "# Particionar em arquivos PARQUET por estado (state)\n",
    "\n",
    "df5.write.partitionBy(\"state\").mode(\"overwrite\").parquet(\"./teste_output_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particionar em arquivos CSV por ano/mes/dia de nascimento (birthDate)\n",
    "# Faltou converter alguns formatos de data\n",
    "\n",
    "df5.write.partitionBy(\"birthDate\").mode(\"overwrite\").parquet(\"./teste_output_csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
